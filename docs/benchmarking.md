# Benchmarking

## Why benchmark

To encourage adoption we need Thrift.Net to be fast and compact. This will be
particularly important for the client runtime but is also relevant for the
compiler.

## What to benchmark

As a minimum we would like to benchmark on the following metrics:

- execution time - how long the operation takes.
- memory usage - how much memory is used by the operation. In a garbage
  collected environment this may also affect execution time.
- memory retention - how much memory is retained after the operation is
  completed. This can be indicative of memory leaks.

Due to the in-progress state of Thrift.Net we will only be able to benchmark
parts of the application as they are written. This gives us the following plan:

1. Build a benchmarking suite for the compiler.
2. Build a benchmarking suite for the Apache runtime library using the generated
   Thrift.Net client code.
3. Build a benchmarking suite for the Thrift.Net runtime library using the
   generated Thrift.Net client code.

Each step of this plan will inform the following steps.

### Against Apache

We want to compare Thrift.Net against the Apache Thrift implementation. Since
the Apache compiler is an executable we should benchmark against a generated
executable of Thrift.Net to allow a fair comparison.

One drawback of this approach is that it will probably only allow us to measure
and compare execution time.

Once the Thrift.Net compiler is complete we can then compare the Apache runtime
using the client code generated by Thrift.Net and the Apache compiler. This will
ensure our generated compiler code does not have any performance problems.

### Against itself

As Thrift.Net is being developed we also need to continue to evaluate it's
performance against itself. This will help protect against performance
regressions. With this approach we can also use a more specialized benchmarking
tool that will allow measurements of memory usage. For this we will use
[BenchmarkDotNet](https://github.com/dotnet/BenchmarkDotNet).

## When we should benchmark

To help prevent performance regressions we should aim to run the benchmarks on
every commit as part of the code review process. This should be ran as a
comparison against a known previous result like the last successful build. It
may be possible to add a comment to each pull request with details of the
benchmarking suite - if so we should investigate this. We can also fail the
build as part of these checks if the performance has regressed by more that a
certain percentage.

We should also consider what platforms to benchmark on and which runtimes we
want to benchmark using. A sensible starting point for these would be:

- Windows 10
- A Linux distribution

We will also want to cover the following runtime:

- The current long term support version of .Net Core (3.1)

## Inspiration

To build our benchmarking suite we will take inspiration from other projects
with successful benchmarking systems. Currently this includes:

- [GraphQL for .NET](https://github.com/graphql-dotnet/graphql-dotnet/tree/master/src/GraphQL.Benchmarks)
- [gRPC client benchmarks](https://github.com/LesnyRumcajs/grpc_bench)
- [Dapper](https://github.com/StackExchange/Dapper/tree/main/benchmarks/Dapper.Tests.Performance)
- [gRPC performance improvements in .NET 5](https://devblogs.microsoft.com/aspnet/grpc-performance-improvements-in-net-5/)

We can also learn from other
[projects that use BenchmarkDotNet](https://github.com/dotnet/BenchmarkDotNet#who-use-benchmarkdotnet).
